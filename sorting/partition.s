// Copyright (C) 2022 Sneller, Inc.
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU Affero General Public License as published by
// the Free Software Foundation, either version 3 of the License, or
// (at your option) any later version.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU Affero General Public License for more details.
//
// You should have received a copy of the GNU Affero General Public License
// along with this program.  If not, see <http://www.gnu.org/licenses/>.

//+build !noasm !appengine

#include "textflag.h"
#include "../vm/bc_imm_amd64.h"

// Autogenerated file. DO NOT EDIT


//func partitionAscAscFloat64(keys *float64, indices *uint64, pivot float64, left int, right int) (unprocessedLeft int, unprocessedRight int)
TEXT ·partitionAscFloat64(SB), NOSPLIT, $0-56
    MOVQ keys+0(FP), SI
    MOVQ indices+8(FP), DI

#define vPivot  Z0
#define keyL    Z1
#define keyR    Z2
#define idxL    Z3
#define idxR    Z4
#define maskL   BX
#define maskR   CX
#define kreg_maskL   K1
#define kreg_maskR   K2

#define Keys    SI
#define Indices DI
#define currL   R8  // bare pointer
#define currR   R9  // bare pointer
#define Left    R10 // index
#define Right   R11 // index

    VBROADCASTSD pivot+16(FP), vPivot

    XORQ maskL, maskL   // maskL = 0
    XORQ maskR, maskR   // maskR = 0

    MOVQ left+24(FP), Left      // currL = keys + 4*left
    LEAQ (Keys)(Left*8), currL

    MOVQ right+32(FP), Right    // currR = keys + 4*(right - 15)
    LEAQ -56(Keys)(Right*8), currR

loop:
    findL:
        TESTQ maskL, maskL
        JNZ findL_end
        findL_loop:
            // currL + 16*4 < currR? (memory regions would not overlap?)
            LEAQ 64(currL), AX
            CMPQ AX, currR
            JGE out_of_data

            LEAQ 8(Left), DX

            VMOVDQU32 (currL), keyL
            VCMPPD $VCMP_IMM_GE_OQ, vPivot, keyL, kreg_maskL // maskL = (keyL >= pivot)

            KTESTW  kreg_maskL, kreg_maskL
            CMOVQEQ AX, currL       // currL += 16*4 if maskL == 0
            CMOVQEQ DX, Left        // Left  += 16   if maskL == 0
            JZ findL_loop           // loop if maskL is zero

        findL_load_indices: // maskL != 0, load indices
            KMOVW kreg_maskL, maskL
            MOVQ currL, AX
            SUBQ Keys, AX   // AX - offset (in bytes)
            VMOVDQU32 (Indices)(AX*1), idxL
    findL_end:

    findR:
        TESTQ maskR, maskR
        JNZ findR_end
        findR_loop:
            // currR - 16*4 > currL? (memory regions would not overlap?)
            LEAQ -64(currR), AX
            CMPQ AX, currL
            JLE out_of_data

            VMOVDQU32 (currR), keyR
            VCMPPD $VCMP_IMM_LE_OQ, vPivot, keyR, kreg_maskR // maskR = (keyR <= pivot)

            LEAQ -8(Right), DX

            // maskR != 0
            KTESTW  kreg_maskR, kreg_maskR
            CMOVQEQ AX, currR       // currR -= 16*4 if maskR == 0
            CMOVQEQ DX, Right       // Right -= 16   if maskR == 0
            JZ findR_loop           // loop if maskR is zero

        findR_load_indices: // maskR != 0, load indices
            KMOVW kreg_maskR, maskR
            MOVQ currR, AX
            SUBQ Keys, AX   // AX - offset (in bytes)
            VMOVDQU32 (Indices)(AX*1), idxR
    findR_end:

    // at this point we will never get maskL == 0 or maskR == 0

align_masks:
#define maskLswap       K4
#define maskRswap       K5
        MOVL $-1, AX            // for example
                                // maskL = 1010'1101'0001'1100    (8 bits set)
                                // masLR = 0000'0111'0000'1001    (5 bits set)

        PEXTL maskL, AX, R12    // R12   = 0000'0000'1111'1111
        PEXTL maskR, AX, R13    // R13   = 0000'0000'0001'1111

        ANDQ R12, R13           // R13   = 0000'0000'0001'1111
        MOVQ R13, AX

        PDEPL maskL, AX, R12    // R12   = 0000'0101'0001'1100    (5 bits set)
        PDEPL maskR, AX, R13    // R13   = 0000'0111'0000'1001    (5 bits set)

        XORQ R12, maskL         // maskL = 1010'1100'0000'0000    (3 bits left)
        XORQ R13, maskR         // maskR = 0000'0000'0000'0000    (no bits left)

        KMOVW R12, maskLswap
        KMOVW R13, maskRswap

swap_elements:
#define keyLcompressed  Z5
#define idxLcompressed  Z6
#define keyRcompressed  Z7
#define idxRcompressed  Z8
    VCOMPRESSPD keyL, maskLswap, keyLcompressed
    VCOMPRESSPD idxL, maskLswap, idxLcompressed
    VCOMPRESSPD keyR, maskRswap, keyRcompressed
    VCOMPRESSPD idxR, maskRswap, idxRcompressed

    VEXPANDPD keyLcompressed, maskRswap, keyR
    VEXPANDPD idxLcompressed, maskRswap, idxR
    VEXPANDPD keyRcompressed, maskLswap, keyL
    VEXPANDPD idxRcompressed, maskLswap, idxL
#undef keyLcompressed
#undef idxLcompressed
#undef keyRcompressed
#undef idxRcompressed
#undef maskLswap
#undef maskRswap

retire_registers:
    MOVQ maskL, AX
    ORQ maskR, AX
    JNZ retire_L_or_R

retire_L_and_R: // maskL == maskR == 0
    MOVQ currL, AX
    SUBQ Keys, AX
    VMOVDQU32 keyL, (currL)
    VMOVDQU32 idxL, (Indices)(AX*1)
    ADDQ $64, currL
    ADDQ $8, Left

    MOVQ currR, DX
    SUBQ Keys, DX
    VMOVDQU32 keyR, (currR)
    VMOVDQU32 idxR, (Indices)(DX*1)
    SUBQ $64, currR
    SUBQ $8, Right

    JMP findL_loop

retire_L_or_R: // either maskL == 0 or maskR == 0
    TESTQ maskL, maskL
    JNZ saveR
    saveL:
        MOVQ currL, AX
        SUBQ Keys, AX
        VMOVDQU32 keyL, (currL)
        VMOVDQU32 idxL, (Indices)(AX*1)
        ADDQ $64, currL
        ADDQ $8, Left
    saveL_end:
    JMP loop // maskR cannot be zero here

    saveR:
        MOVQ currR, AX
        SUBQ Keys, AX
        VMOVDQU32 keyR, (currR)
        VMOVDQU32 idxR, (Indices)(AX*1)
        SUBQ $64, currR
        SUBQ $8, Right
    saveR_end:

    JMP loop

out_of_data:
    // there might be some unsaved data in the left or right register (or both)
    saveL2:
    TESTQ maskL, maskL
    JZ saveL2_end
        MOVQ currL, AX
        SUBQ Keys, AX
        VMOVDQU32 keyL, (currL)
        VMOVDQU32 idxL, (Indices)(AX*1)
    saveL2_end:

    saveR2:
    TESTQ maskR, maskR
    JZ saveR2_end
        MOVQ currR, AX
        SUBQ Keys, AX
        VMOVDQU32 keyR, (currR)
        VMOVDQU32 idxR, (Indices)(AX*1)
    saveR2_end:

return:
    MOVQ Left, unprocessedLeft+40(FP)
    MOVQ Right, unprocessedRight+48(FP)

    RET
#undef vPivot
#undef keyL
#undef keyR
#undef idxL
#undef idxR
#undef maskL
#undef maskR
#undef kreg_maskL
#undef kreg_maskR
#undef Keys
#undef Indices
#undef currL
#undef currR
#undef Left
#undef Right

//func partitionAscDescFloat64(keys *float64, indices *uint64, pivot float64, left int, right int) (unprocessedLeft int, unprocessedRight int)
TEXT ·partitionDescFloat64(SB), NOSPLIT, $0-56
    MOVQ keys+0(FP), SI
    MOVQ indices+8(FP), DI

#define vPivot  Z0
#define keyL    Z1
#define keyR    Z2
#define idxL    Z3
#define idxR    Z4
#define maskL   BX
#define maskR   CX
#define kreg_maskL   K1
#define kreg_maskR   K2

#define Keys    SI
#define Indices DI
#define currL   R8  // bare pointer
#define currR   R9  // bare pointer
#define Left    R10 // index
#define Right   R11 // index

    VBROADCASTSD pivot+16(FP), vPivot

    XORQ maskL, maskL   // maskL = 0
    XORQ maskR, maskR   // maskR = 0

    MOVQ left+24(FP), Left      // currL = keys + 4*left
    LEAQ (Keys)(Left*8), currL

    MOVQ right+32(FP), Right    // currR = keys + 4*(right - 15)
    LEAQ -56(Keys)(Right*8), currR

loop:
    findL:
        TESTQ maskL, maskL
        JNZ findL_end
        findL_loop:
            // currL + 16*4 < currR? (memory regions would not overlap?)
            LEAQ 64(currL), AX
            CMPQ AX, currR
            JGE out_of_data

            LEAQ 8(Left), DX

            VMOVDQU32 (currL), keyL
            VCMPPD $VCMP_IMM_LE_OQ, vPivot, keyL, kreg_maskL // maskL = (keyL >= pivot)

            KTESTW  kreg_maskL, kreg_maskL
            CMOVQEQ AX, currL       // currL += 16*4 if maskL == 0
            CMOVQEQ DX, Left        // Left  += 16   if maskL == 0
            JZ findL_loop           // loop if maskL is zero

        findL_load_indices: // maskL != 0, load indices
            KMOVW kreg_maskL, maskL
            MOVQ currL, AX
            SUBQ Keys, AX   // AX - offset (in bytes)
            VMOVDQU32 (Indices)(AX*1), idxL
    findL_end:

    findR:
        TESTQ maskR, maskR
        JNZ findR_end
        findR_loop:
            // currR - 16*4 > currL? (memory regions would not overlap?)
            LEAQ -64(currR), AX
            CMPQ AX, currL
            JLE out_of_data

            VMOVDQU32 (currR), keyR
            VCMPPD $VCMP_IMM_GE_OQ, vPivot, keyR, kreg_maskR // maskR = (keyR <= pivot)

            LEAQ -8(Right), DX

            // maskR != 0
            KTESTW  kreg_maskR, kreg_maskR
            CMOVQEQ AX, currR       // currR -= 16*4 if maskR == 0
            CMOVQEQ DX, Right       // Right -= 16   if maskR == 0
            JZ findR_loop           // loop if maskR is zero

        findR_load_indices: // maskR != 0, load indices
            KMOVW kreg_maskR, maskR
            MOVQ currR, AX
            SUBQ Keys, AX   // AX - offset (in bytes)
            VMOVDQU32 (Indices)(AX*1), idxR
    findR_end:

    // at this point we will never get maskL == 0 or maskR == 0

align_masks:
#define maskLswap       K4
#define maskRswap       K5
        MOVL $-1, AX            // for example
                                // maskL = 1010'1101'0001'1100    (8 bits set)
                                // masLR = 0000'0111'0000'1001    (5 bits set)

        PEXTL maskL, AX, R12    // R12   = 0000'0000'1111'1111
        PEXTL maskR, AX, R13    // R13   = 0000'0000'0001'1111

        ANDQ R12, R13           // R13   = 0000'0000'0001'1111
        MOVQ R13, AX

        PDEPL maskL, AX, R12    // R12   = 0000'0101'0001'1100    (5 bits set)
        PDEPL maskR, AX, R13    // R13   = 0000'0111'0000'1001    (5 bits set)

        XORQ R12, maskL         // maskL = 1010'1100'0000'0000    (3 bits left)
        XORQ R13, maskR         // maskR = 0000'0000'0000'0000    (no bits left)

        KMOVW R12, maskLswap
        KMOVW R13, maskRswap

swap_elements:
#define keyLcompressed  Z5
#define idxLcompressed  Z6
#define keyRcompressed  Z7
#define idxRcompressed  Z8
    VCOMPRESSPD keyL, maskLswap, keyLcompressed
    VCOMPRESSPD idxL, maskLswap, idxLcompressed
    VCOMPRESSPD keyR, maskRswap, keyRcompressed
    VCOMPRESSPD idxR, maskRswap, idxRcompressed

    VEXPANDPD keyLcompressed, maskRswap, keyR
    VEXPANDPD idxLcompressed, maskRswap, idxR
    VEXPANDPD keyRcompressed, maskLswap, keyL
    VEXPANDPD idxRcompressed, maskLswap, idxL
#undef keyLcompressed
#undef idxLcompressed
#undef keyRcompressed
#undef idxRcompressed
#undef maskLswap
#undef maskRswap

retire_registers:
    MOVQ maskL, AX
    ORQ maskR, AX
    JNZ retire_L_or_R

retire_L_and_R: // maskL == maskR == 0
    MOVQ currL, AX
    SUBQ Keys, AX
    VMOVDQU32 keyL, (currL)
    VMOVDQU32 idxL, (Indices)(AX*1)
    ADDQ $64, currL
    ADDQ $8, Left

    MOVQ currR, DX
    SUBQ Keys, DX
    VMOVDQU32 keyR, (currR)
    VMOVDQU32 idxR, (Indices)(DX*1)
    SUBQ $64, currR
    SUBQ $8, Right

    JMP findL_loop

retire_L_or_R: // either maskL == 0 or maskR == 0
    TESTQ maskL, maskL
    JNZ saveR
    saveL:
        MOVQ currL, AX
        SUBQ Keys, AX
        VMOVDQU32 keyL, (currL)
        VMOVDQU32 idxL, (Indices)(AX*1)
        ADDQ $64, currL
        ADDQ $8, Left
    saveL_end:
    JMP loop // maskR cannot be zero here

    saveR:
        MOVQ currR, AX
        SUBQ Keys, AX
        VMOVDQU32 keyR, (currR)
        VMOVDQU32 idxR, (Indices)(AX*1)
        SUBQ $64, currR
        SUBQ $8, Right
    saveR_end:

    JMP loop

out_of_data:
    // there might be some unsaved data in the left or right register (or both)
    saveL2:
    TESTQ maskL, maskL
    JZ saveL2_end
        MOVQ currL, AX
        SUBQ Keys, AX
        VMOVDQU32 keyL, (currL)
        VMOVDQU32 idxL, (Indices)(AX*1)
    saveL2_end:

    saveR2:
    TESTQ maskR, maskR
    JZ saveR2_end
        MOVQ currR, AX
        SUBQ Keys, AX
        VMOVDQU32 keyR, (currR)
        VMOVDQU32 idxR, (Indices)(AX*1)
    saveR2_end:

return:
    MOVQ Left, unprocessedLeft+40(FP)
    MOVQ Right, unprocessedRight+48(FP)

    RET
#undef vPivot
#undef keyL
#undef keyR
#undef idxL
#undef idxR
#undef maskL
#undef maskR
#undef kreg_maskL
#undef kreg_maskR
#undef Keys
#undef Indices
#undef currL
#undef currR
#undef Left
#undef Right

//func partitionAscAscUint64(keys *uint64, indices *uint64, pivot uint64, left int, right int) (unprocessedLeft int, unprocessedRight int)
TEXT ·partitionAscUint64(SB), NOSPLIT, $0-56
    MOVQ keys+0(FP), SI
    MOVQ indices+8(FP), DI

#define vPivot  Z0
#define keyL    Z1
#define keyR    Z2
#define idxL    Z3
#define idxR    Z4
#define maskL   BX
#define maskR   CX
#define kreg_maskL   K1
#define kreg_maskR   K2

#define Keys    SI
#define Indices DI
#define currL   R8  // bare pointer
#define currR   R9  // bare pointer
#define Left    R10 // index
#define Right   R11 // index

    VPBROADCASTQ pivot+16(FP), vPivot

    XORQ maskL, maskL   // maskL = 0
    XORQ maskR, maskR   // maskR = 0

    MOVQ left+24(FP), Left      // currL = keys + 4*left
    LEAQ (Keys)(Left*8), currL

    MOVQ right+32(FP), Right    // currR = keys + 4*(right - 15)
    LEAQ -56(Keys)(Right*8), currR

loop:
    findL:
        TESTQ maskL, maskL
        JNZ findL_end
        findL_loop:
            // currL + 16*4 < currR? (memory regions would not overlap?)
            LEAQ 64(currL), AX
            CMPQ AX, currR
            JGE out_of_data

            LEAQ 8(Left), DX

            VMOVDQU32 (currL), keyL
            VPCMPUQ $VPCMP_IMM_GE, vPivot, keyL, kreg_maskL // maskL = (keyL >= pivot)

            KTESTW  kreg_maskL, kreg_maskL
            CMOVQEQ AX, currL       // currL += 16*4 if maskL == 0
            CMOVQEQ DX, Left        // Left  += 16   if maskL == 0
            JZ findL_loop           // loop if maskL is zero

        findL_load_indices: // maskL != 0, load indices
            KMOVW kreg_maskL, maskL
            MOVQ currL, AX
            SUBQ Keys, AX   // AX - offset (in bytes)
            VMOVDQU32 (Indices)(AX*1), idxL
    findL_end:

    findR:
        TESTQ maskR, maskR
        JNZ findR_end
        findR_loop:
            // currR - 16*4 > currL? (memory regions would not overlap?)
            LEAQ -64(currR), AX
            CMPQ AX, currL
            JLE out_of_data

            VMOVDQU32 (currR), keyR
            VPCMPUQ $VPCMP_IMM_LE, vPivot, keyR, kreg_maskR // maskR = (keyR <= pivot)

            LEAQ -8(Right), DX

            // maskR != 0
            KTESTW  kreg_maskR, kreg_maskR
            CMOVQEQ AX, currR       // currR -= 16*4 if maskR == 0
            CMOVQEQ DX, Right       // Right -= 16   if maskR == 0
            JZ findR_loop           // loop if maskR is zero

        findR_load_indices: // maskR != 0, load indices
            KMOVW kreg_maskR, maskR
            MOVQ currR, AX
            SUBQ Keys, AX   // AX - offset (in bytes)
            VMOVDQU32 (Indices)(AX*1), idxR
    findR_end:

    // at this point we will never get maskL == 0 or maskR == 0

align_masks:
#define maskLswap       K4
#define maskRswap       K5
        MOVL $-1, AX            // for example
                                // maskL = 1010'1101'0001'1100    (8 bits set)
                                // masLR = 0000'0111'0000'1001    (5 bits set)

        PEXTL maskL, AX, R12    // R12   = 0000'0000'1111'1111
        PEXTL maskR, AX, R13    // R13   = 0000'0000'0001'1111

        ANDQ R12, R13           // R13   = 0000'0000'0001'1111
        MOVQ R13, AX

        PDEPL maskL, AX, R12    // R12   = 0000'0101'0001'1100    (5 bits set)
        PDEPL maskR, AX, R13    // R13   = 0000'0111'0000'1001    (5 bits set)

        XORQ R12, maskL         // maskL = 1010'1100'0000'0000    (3 bits left)
        XORQ R13, maskR         // maskR = 0000'0000'0000'0000    (no bits left)

        KMOVW R12, maskLswap
        KMOVW R13, maskRswap

swap_elements:
#define keyLcompressed  Z5
#define idxLcompressed  Z6
#define keyRcompressed  Z7
#define idxRcompressed  Z8
    VPCOMPRESSQ keyL, maskLswap, keyLcompressed
    VPCOMPRESSQ idxL, maskLswap, idxLcompressed
    VPCOMPRESSQ keyR, maskRswap, keyRcompressed
    VPCOMPRESSQ idxR, maskRswap, idxRcompressed

    VPEXPANDQ keyLcompressed, maskRswap, keyR
    VPEXPANDQ idxLcompressed, maskRswap, idxR
    VPEXPANDQ keyRcompressed, maskLswap, keyL
    VPEXPANDQ idxRcompressed, maskLswap, idxL
#undef keyLcompressed
#undef idxLcompressed
#undef keyRcompressed
#undef idxRcompressed
#undef maskLswap
#undef maskRswap

retire_registers:
    MOVQ maskL, AX
    ORQ maskR, AX
    JNZ retire_L_or_R

retire_L_and_R: // maskL == maskR == 0
    MOVQ currL, AX
    SUBQ Keys, AX
    VMOVDQU32 keyL, (currL)
    VMOVDQU32 idxL, (Indices)(AX*1)
    ADDQ $64, currL
    ADDQ $8, Left

    MOVQ currR, DX
    SUBQ Keys, DX
    VMOVDQU32 keyR, (currR)
    VMOVDQU32 idxR, (Indices)(DX*1)
    SUBQ $64, currR
    SUBQ $8, Right

    JMP findL_loop

retire_L_or_R: // either maskL == 0 or maskR == 0
    TESTQ maskL, maskL
    JNZ saveR
    saveL:
        MOVQ currL, AX
        SUBQ Keys, AX
        VMOVDQU32 keyL, (currL)
        VMOVDQU32 idxL, (Indices)(AX*1)
        ADDQ $64, currL
        ADDQ $8, Left
    saveL_end:
    JMP loop // maskR cannot be zero here

    saveR:
        MOVQ currR, AX
        SUBQ Keys, AX
        VMOVDQU32 keyR, (currR)
        VMOVDQU32 idxR, (Indices)(AX*1)
        SUBQ $64, currR
        SUBQ $8, Right
    saveR_end:

    JMP loop

out_of_data:
    // there might be some unsaved data in the left or right register (or both)
    saveL2:
    TESTQ maskL, maskL
    JZ saveL2_end
        MOVQ currL, AX
        SUBQ Keys, AX
        VMOVDQU32 keyL, (currL)
        VMOVDQU32 idxL, (Indices)(AX*1)
    saveL2_end:

    saveR2:
    TESTQ maskR, maskR
    JZ saveR2_end
        MOVQ currR, AX
        SUBQ Keys, AX
        VMOVDQU32 keyR, (currR)
        VMOVDQU32 idxR, (Indices)(AX*1)
    saveR2_end:

return:
    MOVQ Left, unprocessedLeft+40(FP)
    MOVQ Right, unprocessedRight+48(FP)

    RET
#undef vPivot
#undef keyL
#undef keyR
#undef idxL
#undef idxR
#undef maskL
#undef maskR
#undef kreg_maskL
#undef kreg_maskR
#undef Keys
#undef Indices
#undef currL
#undef currR
#undef Left
#undef Right

//func partitionAscDescUint64(keys *uint64, indices *uint64, pivot uint64, left int, right int) (unprocessedLeft int, unprocessedRight int)
TEXT ·partitionDescUint64(SB), NOSPLIT, $0-56
    MOVQ keys+0(FP), SI
    MOVQ indices+8(FP), DI

#define vPivot  Z0
#define keyL    Z1
#define keyR    Z2
#define idxL    Z3
#define idxR    Z4
#define maskL   BX
#define maskR   CX
#define kreg_maskL   K1
#define kreg_maskR   K2

#define Keys    SI
#define Indices DI
#define currL   R8  // bare pointer
#define currR   R9  // bare pointer
#define Left    R10 // index
#define Right   R11 // index

    VPBROADCASTQ pivot+16(FP), vPivot

    XORQ maskL, maskL   // maskL = 0
    XORQ maskR, maskR   // maskR = 0

    MOVQ left+24(FP), Left      // currL = keys + 4*left
    LEAQ (Keys)(Left*8), currL

    MOVQ right+32(FP), Right    // currR = keys + 4*(right - 15)
    LEAQ -56(Keys)(Right*8), currR

loop:
    findL:
        TESTQ maskL, maskL
        JNZ findL_end
        findL_loop:
            // currL + 16*4 < currR? (memory regions would not overlap?)
            LEAQ 64(currL), AX
            CMPQ AX, currR
            JGE out_of_data

            LEAQ 8(Left), DX

            VMOVDQU32 (currL), keyL
            VPCMPUQ $VPCMP_IMM_LE, vPivot, keyL, kreg_maskL // maskL = (keyL >= pivot)

            KTESTW  kreg_maskL, kreg_maskL
            CMOVQEQ AX, currL       // currL += 16*4 if maskL == 0
            CMOVQEQ DX, Left        // Left  += 16   if maskL == 0
            JZ findL_loop           // loop if maskL is zero

        findL_load_indices: // maskL != 0, load indices
            KMOVW kreg_maskL, maskL
            MOVQ currL, AX
            SUBQ Keys, AX   // AX - offset (in bytes)
            VMOVDQU32 (Indices)(AX*1), idxL
    findL_end:

    findR:
        TESTQ maskR, maskR
        JNZ findR_end
        findR_loop:
            // currR - 16*4 > currL? (memory regions would not overlap?)
            LEAQ -64(currR), AX
            CMPQ AX, currL
            JLE out_of_data

            VMOVDQU32 (currR), keyR
            VPCMPUQ $VPCMP_IMM_GE, vPivot, keyR, kreg_maskR // maskR = (keyR <= pivot)

            LEAQ -8(Right), DX

            // maskR != 0
            KTESTW  kreg_maskR, kreg_maskR
            CMOVQEQ AX, currR       // currR -= 16*4 if maskR == 0
            CMOVQEQ DX, Right       // Right -= 16   if maskR == 0
            JZ findR_loop           // loop if maskR is zero

        findR_load_indices: // maskR != 0, load indices
            KMOVW kreg_maskR, maskR
            MOVQ currR, AX
            SUBQ Keys, AX   // AX - offset (in bytes)
            VMOVDQU32 (Indices)(AX*1), idxR
    findR_end:

    // at this point we will never get maskL == 0 or maskR == 0

align_masks:
#define maskLswap       K4
#define maskRswap       K5
        MOVL $-1, AX            // for example
                                // maskL = 1010'1101'0001'1100    (8 bits set)
                                // masLR = 0000'0111'0000'1001    (5 bits set)

        PEXTL maskL, AX, R12    // R12   = 0000'0000'1111'1111
        PEXTL maskR, AX, R13    // R13   = 0000'0000'0001'1111

        ANDQ R12, R13           // R13   = 0000'0000'0001'1111
        MOVQ R13, AX

        PDEPL maskL, AX, R12    // R12   = 0000'0101'0001'1100    (5 bits set)
        PDEPL maskR, AX, R13    // R13   = 0000'0111'0000'1001    (5 bits set)

        XORQ R12, maskL         // maskL = 1010'1100'0000'0000    (3 bits left)
        XORQ R13, maskR         // maskR = 0000'0000'0000'0000    (no bits left)

        KMOVW R12, maskLswap
        KMOVW R13, maskRswap

swap_elements:
#define keyLcompressed  Z5
#define idxLcompressed  Z6
#define keyRcompressed  Z7
#define idxRcompressed  Z8
    VPCOMPRESSQ keyL, maskLswap, keyLcompressed
    VPCOMPRESSQ idxL, maskLswap, idxLcompressed
    VPCOMPRESSQ keyR, maskRswap, keyRcompressed
    VPCOMPRESSQ idxR, maskRswap, idxRcompressed

    VPEXPANDQ keyLcompressed, maskRswap, keyR
    VPEXPANDQ idxLcompressed, maskRswap, idxR
    VPEXPANDQ keyRcompressed, maskLswap, keyL
    VPEXPANDQ idxRcompressed, maskLswap, idxL
#undef keyLcompressed
#undef idxLcompressed
#undef keyRcompressed
#undef idxRcompressed
#undef maskLswap
#undef maskRswap

retire_registers:
    MOVQ maskL, AX
    ORQ maskR, AX
    JNZ retire_L_or_R

retire_L_and_R: // maskL == maskR == 0
    MOVQ currL, AX
    SUBQ Keys, AX
    VMOVDQU32 keyL, (currL)
    VMOVDQU32 idxL, (Indices)(AX*1)
    ADDQ $64, currL
    ADDQ $8, Left

    MOVQ currR, DX
    SUBQ Keys, DX
    VMOVDQU32 keyR, (currR)
    VMOVDQU32 idxR, (Indices)(DX*1)
    SUBQ $64, currR
    SUBQ $8, Right

    JMP findL_loop

retire_L_or_R: // either maskL == 0 or maskR == 0
    TESTQ maskL, maskL
    JNZ saveR
    saveL:
        MOVQ currL, AX
        SUBQ Keys, AX
        VMOVDQU32 keyL, (currL)
        VMOVDQU32 idxL, (Indices)(AX*1)
        ADDQ $64, currL
        ADDQ $8, Left
    saveL_end:
    JMP loop // maskR cannot be zero here

    saveR:
        MOVQ currR, AX
        SUBQ Keys, AX
        VMOVDQU32 keyR, (currR)
        VMOVDQU32 idxR, (Indices)(AX*1)
        SUBQ $64, currR
        SUBQ $8, Right
    saveR_end:

    JMP loop

out_of_data:
    // there might be some unsaved data in the left or right register (or both)
    saveL2:
    TESTQ maskL, maskL
    JZ saveL2_end
        MOVQ currL, AX
        SUBQ Keys, AX
        VMOVDQU32 keyL, (currL)
        VMOVDQU32 idxL, (Indices)(AX*1)
    saveL2_end:

    saveR2:
    TESTQ maskR, maskR
    JZ saveR2_end
        MOVQ currR, AX
        SUBQ Keys, AX
        VMOVDQU32 keyR, (currR)
        VMOVDQU32 idxR, (Indices)(AX*1)
    saveR2_end:

return:
    MOVQ Left, unprocessedLeft+40(FP)
    MOVQ Right, unprocessedRight+48(FP)

    RET
#undef vPivot
#undef keyL
#undef keyR
#undef idxL
#undef idxR
#undef maskL
#undef maskR
#undef kreg_maskL
#undef kreg_maskR
#undef Keys
#undef Indices
#undef currL
#undef currR
#undef Left
#undef Right
